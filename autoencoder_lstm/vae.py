# -*- coding: utf-8 -*-
"""VAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r5Aeoz8tZo7qIda6b6odiuvlLfWJBeWY
"""

import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import os

import sys
sys.path.append('/content/narouresearch')

from conversion.convert import char2ID, ID2char

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.manual_seed(1)
random_state = 0

BOS = 0
EOS = 1
UNK = 2

class Encoder(nn.Module):
  def __init__(self, input_size, embedding_size, hidden_size):
    # input_size: 入力のuniqueな単語数
    # hidden_size: 隠れ層のユニット数
    super(Encoder, self).__init__()
    self.embedding_size = embedding_size
    self.input_size     = input_size
    self.hidden_size    = hidden_size
    self.embedding      = nn.Embedding(input_size, embedding_size) # paddingなし
    self.LSTM           = nn.LSTM(embedding_size, hidden_size, num_layers=1) # input_sizeはともかく隠れ層のユニット数hiddenでいいのか？

  def forward(self, batch, hidden=None):
    embedding = self.embedding(batch) # 文字のベクトル表現を取ってくる？
    _, hidden = self.LSTM(embedding, hidden)
    return hidden

class Decoder(nn.Module):
  def __init__(self, embedding_size, hidden_size, output_size):
    #vocab_size: 出力のuniqueな単語数
    super(Decoder, self).__init__()
    self.embedding_size = embedding_size
    self.hidden_size    = hidden_size
    self.output_size    = output_size
    self.embedding      = nn.Embedding(output_size, embedding_size)
    self.LSTM           = nn.LSTM(embedding_size, hidden_size, num_layers=1)
    self.out            = nn.Linear(hidden_size, output_size)

  def forward(self, batch, hidden):
    embedding = self.embedding(batch)
    output, hidden = self.LSTM(embedding, hidden)
    output = self.out(output)
    return output, hidden

# 実装簡略化してるところあり
class VAE(nn.Module): # encoder_hidden -> decoder_hidden 
  def __init__(self, input_size, output_size, dimension=2):
    super(VAE, self).__init__()
    self.input_size  = input_size # encoder hidden
    self.dimension = dimension
    self.output_size = output_size # decoder hidden
    self.mu_generator = nn.Linear(input_size, dimension)
    nn.init.xavier_normal_(self.mu_generator.weight) # init weight
    self.sigma_generator = nn.Linear(input_size, dimension)
    nn.init.xavier_normal_(self.sigma_generator.weight) # init weight
    self.out_generator = nn.Linear(dimension, output_size)
    nn.init.xavier_normal_(self.out_generator.weight) # init weight
  
  def forward(self, input_hidden):
    # input_hidden = F.softplus(input_hidden)
    mu = self.mu_generator(input_hidden)
    log_sigma = self.sigma_generator(input_hidden) # log(sigma^2)
    epsilon = torch.randn(self.dimension,1,device=device).contiguous().view(-1)
    z = epsilon * torch.exp(log_sigma / 2) + mu # z = N(mu, sigma)
    output_hidden = self.out_generator(z)
    return output_hidden, mu, log_sigma

class EncoderDecoderWithVAE(nn.Module):
  def __init__(self, input_size, embedding_size, hidden_size, output_size, teacher_forcing_rate=0.5,
               dimension=2):
    super(EncoderDecoderWithVAE, self).__init__()
    self.input_size = input_size
    self.embedding_size = embedding_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.teacher_forcing_rate = teacher_forcing_rate
    self.dimension = dimension
    self.encoder = Encoder(input_size, embedding_size, hidden_size)
    self.VAE = VAE(hidden_size * 2, hidden_size * 2, dimension)
    self.decoder = Decoder(embedding_size, hidden_size, output_size)

  def gen(self, seq, max_length=-1):
    if(max_length < 0): max_length = len(seq)
    encoder_hidden = None
    output_len = max_length
    seq = [chr_id.view(-1,1) for chr_id in seq]
    for chr_id in seq:
      encoder_input = chr_id
      encoder_hidden = self.encoder(encoder_input, encoder_hidden)

    # ([[[1, 1, hidden_size]]], [[[1, 1, hidden_size]]]) -> [1, hidden_size*2]
    encoder_hidden = torch.cat((encoder_hidden[0].view(-1), encoder_hidden[1].view(-1)))
    decoder_hidden, _, _ = self.VAE(encoder_hidden)
    decoder_hidden = (decoder_hidden[:self.hidden_size].view(1,1,-1), decoder_hidden[self.hidden_size:].view(1,1,-1))
    # [1, hidden_size*2] -> ([[[1, 1, hidden_size]]], [[[1, 1, hidden_size]]])

    decoder_outputs = []
    decoder_input = seq[0]
    for i in range(output_len):
      decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)
      decoder_output = F.softmax(decoder_output, dim=2)
      decoder_input = decoder_output.argmax().view(-1,1)
      output_id = int(decoder_input)
      if (output_id < 32): 
        break
      decoder_outputs.append(output_id)
    return decoder_outputs

  def forward(self, seq):
    encoder_hidden = None
    seq_len = len(seq)
    output_len = seq_len - 1
    seq = [chr_id.view(-1,1) for chr_id in seq]
    for chr_id in seq:
      encoder_input = chr_id
      encoder_hidden = self.encoder(encoder_input, encoder_hidden)
    
    # ([[[1, 1, hidden_size]]], [[[1, 1, hidden_size]]]) -> [1, hidden_size*2]
    encoder_hidden = torch.cat((encoder_hidden[0].view(-1), encoder_hidden[1].view(-1)))
    decoder_hidden, mu, log_sigma = self.VAE(encoder_hidden)
    decoder_hidden = (decoder_hidden[:self.hidden_size].view(1,1,-1), decoder_hidden[self.hidden_size:].view(1,1,-1))
    # [1, hidden_size*2] -> ([[[1, 1, hidden_size]]], [[[1, 1, hidden_size]]])

    decoder_outputs = torch.zeros(output_len, 1, self.output_size, device=device)
    decoder_input = seq[0]
    for i in range(output_len):
      decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)
      decoder_output = F.softmax(decoder_output, dim=2)
      decoder_outputs[i] = decoder_output
      teacher_forcing = (torch.rand(1) < self.teacher_forcing_rate)
      if teacher_forcing: decoder_input = seq[i+1]
      else: decoder_input = decoder_output.argmax().view(-1,1)
    return decoder_outputs, mu, log_sigma

def calc_loss(pred, mu, log_sigma, target): # targetをone-hotと見なしている
  reconstruction_loss = 0
  kld = 0
  target = target[1:]
  pred = pred.view(-1, 26000)
  # reconstruction_loss
  for pred_ids, target_id in zip(pred, target): 
    # xlog(y) + (1 - x)log(1 - y)
    # x = 0 or 1
    # = sum( log(1-y) ) - log(1-y) + log(y) 
    reconstruction_loss += torch.log(1 - pred_ids).sum() + torch.log(pred_ids[target_id]) - torch.log(1 - pred_ids[target_id])
  # Kullback Leibler Divergence
  mu_square = mu * mu
  sigma = torch.exp(log_sigma)
  kld = (1 + log_sigma - mu_square - sigma).sum() * 0.5
  loss = -(reconstruction_loss + kld)
  return loss
